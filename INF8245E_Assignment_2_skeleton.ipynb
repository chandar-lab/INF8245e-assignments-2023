{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLGUexl-F8Jz"
      },
      "source": [
        "## Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMnyaCUtF8J5"
      },
      "source": [
        "## Instructions\n",
        "* This is an individual assignment. You are not allowed to discuss the problems with other students.\n",
        "\n",
        "* Parts of this assignment will be autograded by gradescope. You can use it as immediate feedback to improve your answers. You can resubmit as many times as you want. Please also note that the **public tests are not exhaustive**, meaning that passing the public tests does not necessarily imply that your implementation is correct. It is recommended to test your implementation in your own ways instead of solely relying on the autograder.\n",
        "\n",
        "* All your solution, code, analysis, graphs, explanations should be done in this same notebook.\n",
        "\n",
        "* Please make sure to execute all the cells before you submit the notebook to the gradescope. You will not get points for the plots if they are not generated already.\n",
        "\n",
        "* If you have questions regarding the assignment, you can ask for clarifications in Piazza. You should use the corresponding tag for this assignment.\n",
        "\n",
        "**Warning:** Throughout the assignment, you will be asked to implement certain algorithms. In the solution you submit, do not simply call a library function which performs the entire algorithm for you, this is forbidden, as it would obviously defeat the purpose.  For example, if you were asked to implement the kNN classifier algorithm, do not simply call `sklearn.neighbors.KNeighborsClassifier` on the given data and submit that as your solution.\n",
        "\n",
        "**When Submitting to GradeScope**: Be sure to\n",
        "1) Submit a `.ipynb` notebook to the `Assignment 2 - Practial` section on Gradescope.\n",
        "2) Submit a `pdf` version of the notebook to the `Assignment 2 - Analysis` entry.\n",
        "\n",
        "**Note 1**: The only exceptions to the warning above are _Question 1.1_ and _Question 2.2_ in which you're allowed to the use SciPy's functions. Relevant hints are given to you at certain stages of the assignment to help you.\n",
        "\n",
        "**Note 2**: You can choose to submit responses in either English or French.\n",
        "\n",
        "\n",
        "## Context\n",
        "In this assigment, you are asked to:\n",
        "\n",
        "* Work with the image classification dataset. Specifically, you will implement the following three classifiers and compare their performance on two datasets.\n",
        "  * Implement a k-Nearest Neighbour classifier\n",
        "  * Implement a Gaussian Naive Bayes classifier\n",
        "  * Implement a Logisitic Regression classifier\n",
        "\n",
        "* Derive Poission Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3xPe8ECF8J6"
      },
      "source": [
        "The next two cells install and initialize otter-grader. You should run these cells as they are WITHOUT modifying them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.101610Z",
          "start_time": "2023-09-25T14:39:54.701862Z"
        },
        "id": "AsvF6oTBF8J7"
      },
      "outputs": [],
      "source": [
        "!pip install otter-grader\n",
        "!git clone https://github.com/chandar-lab/INF8245e-assignments-2023.git public"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.101959Z",
          "start_time": "2023-09-25T14:39:56.870149Z"
        },
        "id": "vt2liECeF8J8"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook(colab=True, tests_dir='./public/a2/tests')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjCdiYnWF8J9"
      },
      "source": [
        "The next cells call the necessary libraries for this assignment and contain some helper functions for you to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.102068Z",
          "start_time": "2023-09-25T14:39:56.874508Z"
        },
        "id": "jwHdT53SF8J-"
      },
      "outputs": [],
      "source": [
        "# Import Packages\n",
        "import typing\n",
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Type\n",
        "%matplotlib inline\n",
        "\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "# NOTE: Do NOT change the order of this import call\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "import tensorflow as tf\n",
        "# NOTE: Keras has only been used here to import the dataset, you're\n",
        "# not allowed to use it anywhere else (we haven't either!)\n",
        "\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "rng = np.random.RandomState(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:54:01.812152Z",
          "start_time": "2023-09-25T19:54:01.710583Z"
        },
        "id": "qRV_L0MPF8J-"
      },
      "outputs": [],
      "source": [
        "# Helper Function\n",
        "def iterate_samples(batch_size, sample_set, label_set, shuffle=True):\n",
        "    # set random seed reproducibility\n",
        "    np.random.seed(42)\n",
        "\n",
        "    order = np.arange(sample_set.shape[0])\n",
        "    if shuffle:\n",
        "        np.random.shuffle(order)\n",
        "\n",
        "    for i in range(0, sample_set.shape[0], batch_size):\n",
        "        batch_samples = sample_set[order[i : i + batch_size]]\n",
        "        batch_labels = label_set[order[i : i + batch_size]]\n",
        "        yield batch_samples, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "riZ-AzFn1JKZ"
      },
      "source": [
        "Next, we load the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.102551Z",
          "start_time": "2023-09-25T14:39:56.896422Z"
        },
        "id": "-dRzmj3p1JKZ"
      },
      "outputs": [],
      "source": [
        "# Load  MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "# Shuffle the training set\n",
        "permuted = np.random.permutation(len(x_train))\n",
        "x_train, y_train = x_train[permuted], y_train[permuted]\n",
        "\n",
        "print(f\"Number of images for training: {x_train.shape[0]}\")\n",
        "print(f\"Number of images for testing: {x_test.shape[0]}\")\n",
        "print(f\"Size of  MNIST images: {x_train[0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "A_40umzW1JKZ"
      },
      "source": [
        "Normalize the training and test data as following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.102735Z",
          "start_time": "2023-09-25T14:39:57.231230Z"
        },
        "id": "b08jGv2E1JKZ"
      },
      "outputs": [],
      "source": [
        "# NOTE: We normalize the training data by dividing the input by max value.\n",
        "max_value = np.max(x_train)\n",
        "\n",
        "train_images = x_train / max_value\n",
        "test_images = x_test / max_value\n",
        "\n",
        "# Vectorize the data\n",
        "train_images = train_images.reshape((train_images.shape[0], -1))\n",
        "test_images = test_images.reshape((test_images.shape[0], -1))\n",
        "\n",
        "print(f\"Training inputs' shape after vectorization: {train_images.shape}\")\n",
        "print(f\"Testing inputs' shape after vectorization: {test_images.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "I8QIMNi11JKa"
      },
      "source": [
        "### 1. k-Nearest Neighbour Classification (30 points)\n",
        "\n",
        "In this section, we will be looking at the k-nearest neighbours algorithm for classifiying 10 different categores in the MNIST dataset. In particular, we will learn about the following:\n",
        "\n",
        "1. Implement a function that classifies MNIST using the kNN algorithm.\n",
        "2. Perform a brief cross-validation to get the best k."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5Ji_9zLF8KG"
      },
      "source": [
        "**Question 1.1 (6 points):** Implement the distance function `get_distance`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-27T12:36:45.014272Z",
          "start_time": "2023-09-27T12:36:44.924018Z"
        },
        "id": "bvSUaGZNF8KG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_distance(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Computes the Euclidean distance between two arrays\n",
        "\n",
        "    Args:\n",
        "        A (np.ndarray): Numpy array of shape [num_samples_a x num_features]\n",
        "        B (np.ndarray): Numpy array of shape [num_samples_b x num_features]\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Numpy array of shape [num_samples_a x num_samples_b] where\n",
        "                    each column contains the distance between one element in\n",
        "                    matrix_b and all elements in matrix_a\n",
        "    \"\"\"\n",
        "    # NOTE: Depending on your implementation, chances are that you might get out-of-memory errors on Colab. If that is\n",
        "    # the case, look into the documentation of SciPy's cdist function.\n",
        "\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "t6KgWvoe-QmR"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1cs_Z3u1F8KM"
      },
      "source": [
        "**Question 1.2 (16 points):** Implement `get_k_neighbors` function to the get the labels of the k-nearest neighbours from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.106875Z",
          "start_time": "2023-09-25T14:39:57.373706Z"
        },
        "id": "dsE1loQSF8KN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_k_neighbors(distances: np.ndarray, labels: np.ndarray, k: int) -> np.ndarray:\n",
        "    \"\"\"Gets the k nearest labels based on the distances\n",
        "\n",
        "    Args:\n",
        "        distances (np.ndarray): Numpy array of shape num_train_samples x num_test_samples\n",
        "                                containing the Euclidean distances\n",
        "        labels (np.ndarray): Numpy array of shape [num_train_samples, ] containing\n",
        "                                the training labels\n",
        "        k (int): Number of nearest neighbours\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Numpy array of shape [k x num_test_samples] containing the\n",
        "                    training labels of the k nearest neighbours for each test sample\n",
        "    \"\"\"\n",
        "\n",
        "    # Sort the distances in ascending and get the indices of the first \"k\" elements\n",
        "    # HINT: You need to sort the distances in ascending order to get the indices\n",
        "    # of the first \"k\" elements. BUT, you would not need to sort the entire array,\n",
        "    # it would be enough to make sure that the \"k\"-th element is in the correct position!\n",
        "\n",
        "    # NOTE: Since the matrix sizes are huge, it would be impractical to run any sort of a\n",
        "    # loop to get the nearest labels. Think about how you can do it without using loops.\n",
        "\n",
        "    ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fGzRSdAX-QmS"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1.2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "oVkSsAO8F8KS"
      },
      "source": [
        "**Question 1.3 (6 points):** Implement the `get_prediction` function that returns the label class that occurs most frequently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.107231Z",
          "start_time": "2023-09-25T14:39:57.391489Z"
        },
        "id": "OgMhRJnLF8KT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_prediction(nearest_labels: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Gets the best prediction, i.e. the label class that occurs most frequently\n",
        "\n",
        "    Args:\n",
        "        nearest_labels (np.ndarray): Numpy array of shape [k x num_test_samples] obtained from the output of the get_k_neighbors function\n",
        "\n",
        "    Returns:\n",
        "        np.array: Numpy array of shape [num_test_samples] containing the best prediction for each test sample\n",
        "    \"\"\"\n",
        "\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "mPXPbETj-QmS"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1.3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfY0DUy6F8KW"
      },
      "source": [
        "Now, using these functions, we will run the k-NN classifier on a subset of the MNIST dataset! Particularly, we will use 50,000 samples for training, 10,000 samples for validation, and 10,000 samples for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.107604Z",
          "start_time": "2023-09-25T14:39:57.443151Z"
        },
        "id": "0A3-xsgjF8KX"
      },
      "outputs": [],
      "source": [
        "n_train_samples = 50000\n",
        "n_val_samples = 10000\n",
        "n_test_samples = 10000\n",
        "\n",
        "# define the training set and labels\n",
        "train_set = train_images[:n_train_samples]\n",
        "train_labels = y_train[:n_train_samples]\n",
        "print(f\"Training set shape: {train_set.shape}\")\n",
        "\n",
        "# define the validation set and labels\n",
        "val_set = train_images[-n_val_samples:]\n",
        "val_labels = y_train[-n_val_samples:]\n",
        "print(f\"Validaton set shape: {val_set.shape}\")\n",
        "\n",
        "# define the test set and labels\n",
        "test_set = test_images[:n_test_samples]\n",
        "test_labels = y_test[:n_test_samples]\n",
        "print(f\"Test set shape: {test_set.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:39:58.107695Z",
          "start_time": "2023-09-25T14:39:57.446943Z"
        },
        "id": "5FLJ9bIxF8KX"
      },
      "outputs": [],
      "source": [
        "def knn_classifier(training_set: np.ndarray, training_labels: np.ndarray,\n",
        "                  test_set: np.ndarray, test_labels: np.ndarray, k: int) -> float:\n",
        "  \"\"\"\n",
        "  Performs k-nearest neighbour classification\n",
        "\n",
        "  Args:\n",
        "    training_set (np.ndarray): Vectorized training images (shape: [num_train_samples x num_features])\n",
        "    training_labels (np.ndarray): Training labels (shape: [num_train_samples, 1])\n",
        "    test_set (np.ndarray): Vectorized test images (shape: [num_test_samples x num_features])\n",
        "    test_labels (np.ndarray): Test labels (shape: [num_test_samples, 1])\n",
        "    k (int): number of nearest neighbours\n",
        "\n",
        "  Returns:\n",
        "    accuracy (float): the accuracy in %\n",
        "  \"\"\"\n",
        "\n",
        "  dists = get_distance(A=training_set, B=test_set)\n",
        "\n",
        "  nearest_labels = get_k_neighbors(distances=dists, labels=training_labels, k=k)\n",
        "\n",
        "  # from the nearest labels above choose the label classes that occurs most frequently\n",
        "  predictions = get_prediction(nearest_labels)\n",
        "\n",
        "  # calculate and return accuracy of the predicitions\n",
        "  accuracy = (np.equal(predictions, test_labels).sum())/len(test_set) * 100.0\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8upWdQfF8KY"
      },
      "source": [
        "With the kNN classifier defined, how would you choose the best possible *k* value? We will try three different values of _k_ (2, 4 and 6) and evaluate on the validation set. You are asked to report the k-value which obtains the best validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T14:51:20.961024Z",
          "start_time": "2023-09-25T14:39:57.449944Z"
        },
        "id": "KZkObP77F8KZ"
      },
      "outputs": [],
      "source": [
        "# dictionary to store the k values as keys and the validation accuracies as the values\n",
        "val_accuracy_per_k = {}\n",
        "\n",
        "for k in [2, 4, 6]:\n",
        "    val_accuracy_per_k[k] = knn_classifier(train_set, train_labels, val_set, val_labels, k)\n",
        "\n",
        "best_k = max(val_accuracy_per_k, key=val_accuracy_per_k.get)\n",
        "print(f\"Best validation accuracy of {val_accuracy_per_k[best_k]} % for k={best_k}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fDk6OHNJF8KZ"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question 1.4 (1 point):** Report the best validation accuracy and the corresponding *k* value that achieves it? (1 sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgZHRCQAF8Ka"
      },
      "source": [
        "**Answer 1.4:** .... obtains the best validation accuracy of .... %"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "PIZ9kR81F8Kd"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "#### Reporting the test accuracy (1 points)\n",
        "In this section, you are asked to report the test accuracy for the best-k value obtained in the previous question. You are not required to implement anything here, rather, the points are associated to the test accuracy you will report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-09-25T14:51:20.974116Z"
        },
        "id": "qYupmb_yF8Kd"
      },
      "outputs": [],
      "source": [
        "# Now, based on the best value of k, we run the kNN classifier on the test set.\n",
        "test_accuracy = knn_classifier(train_set, train_labels, test_set, test_labels, k=best_k)\n",
        "print(test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fbB7S0FKF8Ke"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question 1.5 (1 point):** Report the test accuracy obtained with the best k-value (1 sentence)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-mogGCoF8Kf"
      },
      "source": [
        "**Answer 1.5:** The test accuracy with ..... is ..... %."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "J94fcP1BF8Ko"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "### 2.  Classification using Gaussian Naive Bayes (21 points)\n",
        "\n",
        "In this section, we will use the Gaussian Naive Bayes (GNB) classifier on the MNIST dataset. The GNB classifier belongs to the family of probabilistic classifiers based on the application of Bayes' Theorem. The term \"naive\" in naive Bayes classifiers comes from the fact that they have a strong independence assumptions between the features. In particular, it assumes that the value of a particular feature is independent of the value of another feature, _given the class variable_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYLuTaxtF8Kp"
      },
      "source": [
        "Consider the training set $\\{( x^{(1)},y^{(1)} ), \\ldots, ( x^{(N)},y^{(N)} )) \\}$ of N labeled examples, and the input features are $x^{(i)} \\in \\mathbb{R}^n$. Since we are interested in multi-class classification, the label $y$ can take K different values i.e. $y^{(i)} \\in \\{1, 2, \\ldots, K\\}$. The GNB model assumes that the **class-conditional densities** are distributed according to a multi-variate Gaussian distribution. In other words, the probability of observing the data $x^{(i)}$ given the class variable (also known as, _likelihood_) is given by a Gaussian distribution as shown below:\n",
        "\n",
        "$$\n",
        "P(x \\mid y=k, \\mu_k, \\Sigma_k) = \\mathcal{N}(x \\mid \\mu_k, \\Sigma_k)\n",
        "$$\n",
        "where $\\mu_k$ denotes the class-specific mean vector and $\\Sigma_k$ denotes the class-specific covariance matrix (meaning that each class has its own mean vector and the covariance matrix). Note that since we have a separate covariance matrix for each class $k$, the covariance matrices are not shared among all the classes.\n",
        "\n",
        "Given the likelihood of the model, we can now calculate the posterior probability, that is, the probability of a label belonging to a particular class given the data $x^{(i)}$, using Bayes' theorem as follows:\n",
        "\n",
        "$$\n",
        "P(y=k \\mid x, \\mu_k, \\Sigma_k) = \\frac{P (x \\mid y=k, \\mu_k, \\Sigma_k) P(y=k)}{\\sum_{c=1}^K P (x \\mid y=c, \\mu_c, \\Sigma_c) P(y=c)}\n",
        "$$\n",
        "where $P(y=k)$ denotes the prior probability of a label belonging to a particular class. The denominator is essentially a normalization constant and is not technically required to be implemented. Observe that it this likelihood $P (x \\mid y=k, \\mu_k, \\Sigma_k)$ that is distributed according to a multi-variate Gaussian distribution given below:\n",
        "\n",
        "$$\n",
        "P(x \\mid y=k, \\mu_k, \\Sigma_k) = \\frac{1}{(2 \\pi)^{K/2} | \\Sigma_k |^{1/2} } \\exp \\left( -\\frac{1}{2} (x - \\mu_k)^{T} \\Sigma_k^{-1} (x - \\mu_k) \\right)\n",
        "$$\n",
        "where $| \\Sigma_k |$ denotes the determinant of the covariance matrix, and $K$ denotes the number of classes. An important note here is that the probabilities are small and in the case of high dimensionality they tend to be very close to zero and result in numerical underflow issues. Therefore, we will be considering the _log_ of the likelihood function instead. Hence, the resulting log-likelihood can be written as:\n",
        "\n",
        "$$\n",
        "\\log P(x \\mid y=k, \\mu_k, \\Sigma_k) = - \\frac{K}{2} \\log(2 \\pi) - \\frac{1}{2} \\log(| \\Sigma_k |) - \\frac{1}{2} (x - \\mu_k)^{T} \\Sigma_k^{-1} (x - \\mu_k)\n",
        "$$\n",
        "\n",
        "Likewise, the log-posterior can be written as:\n",
        "$$\n",
        "\\log P(y=k \\mid x, \\mu_k, \\Sigma_k) \\propto \\log P (x \\mid y=k, \\mu_k, \\Sigma_k) + \\log P(y=k) - \\log (\\textrm{const.})\n",
        "$$\n",
        "Note that we have used the $\\propto$ symbol above which indicates that the log of the posterior probability density for a class can be computed as the sum of the log-likelihood and the log prior probability densities, _upto_ the log of the normalization constant.\n",
        "\n",
        "Now, given the theory, there are two implementation questions in this section. In the first, you are asked to compute the class-specific mean and covariance vectors and the prior probabilities for each class. In the second question, you are required to use these 3 quantities to calculate the posterior probability of each class given the data. While the math and notations given above maybe a bit overwhelming, you are not required to implement the multi-variate Gaussian function. You can use SciPy's function for the same, more details are given in the question.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3DT25TChF8Kp"
      },
      "source": [
        "**Question 2.1 (10 points):** Complete the `gnb_fit_classifier` function that fits the GNB classifier on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fXYXMe_F8Kq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def gnb_fit_classifier(X: np.ndarray, Y: np.ndarray, smoothing: float=1e-3) -> typing.Tuple:\n",
        "    \"\"\"Fits the GNB classifier on the training data\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): numpy array of shape [num_samples x num_features] containing the training data\n",
        "        Y (np.ndarray): numpy array of shape [num_samples, ] containing the training labels\n",
        "        smoothing (float, optional): constant to avoid division by zero. Defaults to 1e-3.\n",
        "\n",
        "    Returns:\n",
        "        prior_probs (typing.List[float]): list of length `num_classes` containing the prior probabilities of the training labels\n",
        "        means (typing.List[np.ndarray]): list of length `num_classes` containing the means of the batch of samples belonging to a particular label\n",
        "                                            shape of each element in the list - (num_features, )\n",
        "        vars (typing.List[np.ndarray]): list o f length `num_classes` containing the variances of the batch of samples belonging to a particular label\n",
        "                                            shape of each element in the list - (num_features, )\n",
        "    \"\"\"\n",
        "\n",
        "    # to set the prior probability of each label by counting the number of times the label appears in\n",
        "    # training data and normalizing it by the total number of training samples.\n",
        "    prior_probs = []\n",
        "\n",
        "    means, vars = [], []\n",
        "\n",
        "    labels = np.unique(Y)\n",
        "    num_classes = len(labels)\n",
        "\n",
        "    ...\n",
        "\n",
        "    return prior_probs, means, vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "rQmE3USY-QmU"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q2.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_0j5wzgdF8Ks"
      },
      "source": [
        "**Question 2.2 (10 points):** Complete the `gnb_predict` function to get the predictions from the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEBy1OnrF8Ks",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def gnb_predict(X: np.ndarray, prior_probs: typing.List[np.ndarray],\n",
        "                    means: typing.List[np.ndarray], vars: typing.List[np.ndarray], num_classes: int) -> np.ndarray:\n",
        "    \"\"\"Computes the predictions of all test samples from the GNB classifier\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): numpy array of shape [num_samples x features] containing vectorized test images\n",
        "        prior_probs (typing.List[float]): list of length `num_classes` containing the prior probabilities of the training labels\n",
        "        means (typing.List[np.ndarray]): list of length `num_classes` containing the means of the batch of samples belonging to a particular label\n",
        "        vars (typing.List[np.ndarray]): list of length `num_classes` containing the variances of the batch of samples belonging to a particular label\n",
        "        num_classes (int): int defining the number of classes\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: numpy array of shape (num_samples) containing predictions for each test sample\n",
        "    \"\"\"\n",
        "\n",
        "    num_samples, feature_dim = X.shape\n",
        "\n",
        "    all_preds = np.zeros((num_samples,  num_classes))\n",
        "\n",
        "    # HINT: Check out SciPy's multivariate normal documentation and\n",
        "    # think about which function to use to prevent underflow issues\n",
        "\n",
        "    ...\n",
        "\n",
        "    # for each prediction in `all_preds`, get the label the label that occurs most frequently\n",
        "    preds = np.argmax(all_preds, axis=1)\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "oG3MYKDZ-QmU"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q2.2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TapwtOGVF8Ku"
      },
      "source": [
        "Now using the functions above let us test the GNB classifer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0NkBZKuF8Kv"
      },
      "outputs": [],
      "source": [
        "def gnb_classifier(train_set, train_labels, test_set, test_labels, smoothing=1e-3):\n",
        "\n",
        "  num_classes = len(np.unique(y_train))\n",
        "\n",
        "  prior_probs, means, vars = gnb_fit_classifier(train_set, train_labels, smoothing=1e-3)\n",
        "\n",
        "  preds = gnb_predict(test_set, prior_probs, means, vars, num_classes)\n",
        "\n",
        "  accuracy = np.mean(np.equal(preds, test_labels)) * 100.0\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmZvM2PdF8Kv"
      },
      "outputs": [],
      "source": [
        "n_train_samples = 50000\n",
        "n_val_samples = 10000\n",
        "n_test_samples = 10000\n",
        "\n",
        "# define the training set and labels\n",
        "train_set = train_images[:n_train_samples]\n",
        "train_labels = y_train[:n_train_samples]\n",
        "print(f\"Training set shape: {train_set.shape}\")\n",
        "\n",
        "# define the validation set and labels\n",
        "val_set = train_images[-n_val_samples:]\n",
        "val_labels = y_train[-n_val_samples:]\n",
        "print(f\"Validaton set shape: {val_set.shape}\")\n",
        "\n",
        "# define the test set and labels\n",
        "test_set = test_images[:n_test_samples]\n",
        "test_labels = y_test[:n_test_samples]\n",
        "print(f\"Test set shape: {test_set.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "9E3_LSYVF8Kw"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question 2.3 (1 point):** Report the test accuracy obtained by the GNB classifier. (1 sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "BD7g4vWUF8Kw"
      },
      "outputs": [],
      "source": [
        "# test the model!\n",
        "test_acc = gnb_classifier(train_set, train_labels, test_set, test_labels)\n",
        "print(f\"Test accuracy: {test_acc} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igocddp-F8Kx"
      },
      "source": [
        "**Answer 2.3:** The test accuracy obtained by the GNB classifier is .... %."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "midmXGlAF8K1"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "### 3.  Classification using Logistic Regression (43 points)\n",
        "\n",
        "In this section, you will be using logistic regression for classifying different categories on the same MNIST dataset. In particular, the following are the objectives for this section:\n",
        "\n",
        "1. Understanding logistic regression for multi-class classification problems.\n",
        "2. Learning to derive the gradient of the softmax function and implement the `softmax` function.\n",
        "3. Implementing the gradient updates in the function `compute_gradient`.\n",
        "4. Understanding and implementing the training, validation, testing phases in a standard machine learning training regime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECV_DfuKF8K1"
      },
      "source": [
        "\n",
        "Consider a logistic regression model for classifying the MNIST categories, where we have a training set $\\{( x^{(1)},y^{(1)} ), \\ldots, ( x^{(N)},y^{(N)} )) \\}$ of N labeled examples, and the input features are $x^{(i)} \\in \\mathbb{R}^n$.\n",
        "\n",
        "Since we are interested in multi-class classification, the label $y$ can take K different values i.e. $y^{(i)} \\in \\{1, 2, \\ldots, K\\}$. Note that for ease of notation, we start the index of classes from 1, rather than from 0.\n",
        "\n",
        "Now, given a test input $x^{(i)}$, we want our hypothesis to estimate the probability that $P(y=k | x^{(i)})$ for each value of $k = 1, \\ldots , K $, i.e. we want to estimate the probability of the class label taking on each of the K\n",
        "different possible values. Thus, our hypothesis will output a $K - $\n",
        "dimensional vector (whose elements sum to 1) giving us our $K$ estimated probabilities. Concretely, the hypothesis function (denoted by $z$) for a single input $x^{(i)}$ takes the following form:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    z_{(w, b)}(x^{(i)}) &=\n",
        "        \\begin{bmatrix}\n",
        "           P(y=1 \\mid x^{(i)}; w, b) \\\\\n",
        "           P(y=2 \\mid x^{(i)}; w, b) \\\\\n",
        "           \\vdots \\\\\n",
        "           P(y=K \\mid x^{(i)}; w, b)\n",
        "         \\end{bmatrix}\n",
        "          &= \\frac{1}{\\sum_{j=1}^K \\exp(h^{(i)}_j)}  \n",
        "          \\begin{bmatrix}\n",
        "           \\exp(h^{(i)}_1) \\\\\n",
        "           \\exp(h^{(i)}_2) \\\\\n",
        "           \\vdots \\\\\n",
        "           \\exp(h^{(i)}_K))\n",
        "         \\end{bmatrix}\n",
        "  \\end{aligned},\n",
        "$$\n",
        "\n",
        "where $h^{(i)} = x^{(i)}.w + b$ containing the parameters of the model. Particularly,  $w \\in \\mathbb{R}^{n \\times K}$ denotes the weight matrix, $b \\in \\mathbb{R}^K$ is the bias vector associated with each of the classes. Lastly, notice in the hypothesis function that we have a term of the form $\\frac{\\exp(⋅)}{\\sum_j \\exp(⋅)}$, this is called the softmax function and is used frequently in machine learning for multi-class classification problems since it outputs values as probabilities between 0 and 1.\n",
        "\n",
        "\n",
        "We will use the negative log-likelihood loss for training our logistic regression model. As the name suggests, it simply calculates the negative of the log likelihood of the model and is given by:\n",
        "$$\n",
        "L = - \\frac{1}{N} \\sum_{i=1}^N \\log \\hat{y}^{(i)} = - \\frac{1}{N} \\sum_{i=1}^N \\log \\left( \\frac{ e^{h^{(i)}_y} }{ \\sum_{j=1}^K e^{h^{(i)}_j} } \\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "hLqBZ6V1F8K2"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question 3.1 (10 points)**\n",
        "\n",
        "Now that we have defined our model and the loss function, calculate the derivative of the loss function $L$ w.r.t the weight matrix $w$ and the bias vector $b$. In other words, get the gradient update expressions for $w$ i.e. $\\frac{\\partial L}{\\partial w}$ and $b$ i.e. $\\frac{\\partial L}{\\partial b}$. Make sure the shapes of all matrices involved are consistent as given in the description above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhO3llsnF8K2"
      },
      "source": [
        "**Hints**:\n",
        "\n",
        "1. It might be good to start by calculating the derivative of the softmax function.\n",
        "2. Think about how you can get the derivative of a particular quantity when it is inside a summation. Is there a way to divide the term into sub-terms and then computing the individual gradients?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHS7PUroF8K2"
      },
      "source": [
        "**Answer 3.1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "vHUUu9ooF8K4"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "With all the theory in place, we will now implement individual functions at bring them all together at the end to train your logistic regression model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "WmfUK1o9F8K4"
      },
      "source": [
        "**Question 3.2 (2 Points)**\n",
        "Implement the `softmax` function below. As we have seen in the introduction, the softmax function is given by:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\textrm{softmax}(x) = \\frac{\\exp(x)}{ \\sum_{j} \\exp{(x_j)}}\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEGz8D6TF8K4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def softmax(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Takes the input and applies the softmax function to it\n",
        "\n",
        "    Args:\n",
        "        x: Numpy array\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: the softmax-ed input\n",
        "    \"\"\"\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "FSdCvtVf-QmW"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q3.2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "NRUF0RQcF8K7"
      },
      "source": [
        "**Question 3.3 (3 points)**\n",
        "A skeleton of the logistic regression model is given to you in the `LogisticRegressionModel` class. Implement the `__call__` function that essentially computes the output probability given the batch of inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:06.275535Z",
          "start_time": "2023-09-25T19:55:06.232048Z"
        },
        "id": "foHpJ5jMF8K7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class LogisticRegressionModel:\n",
        "    def __init__(self, init_weights: np.ndarray) -> None:\n",
        "        num_classes = init_weights.shape[1]\n",
        "        # the weight matrix. Shape = [num_features, num_classes]\n",
        "        self.W = np.copy(init_weights)\n",
        "        # the bias vector. Shape = [num_classes]\n",
        "        self.b = np.zeros((num_classes))\n",
        "\n",
        "    def __call__(self, x: np.ndarray) -> np.ndarray :\n",
        "        \"\"\"\n",
        "        Computes the hypothesis function, i.e. the prediction (y_hat) of the logistic regression model\n",
        "\n",
        "        Args:\n",
        "            x: Numpy array of shape [batch_size x num_features] containing input mini-batch of samples\n",
        "\n",
        "        Returns:\n",
        "            x: Numpy array of shape (shape: [batch_size x num_classes]) containing the output class probabilities\n",
        "                after applying the softmax function\n",
        "        \"\"\"\n",
        "        # HINT: Look into the documentation of np.matmul\n",
        "\n",
        "        ...\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3j6KqYXp-QmW"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q3.3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6Gz43GBjF8K9"
      },
      "source": [
        "**Question 3.4 (3 points)**\n",
        "Implement the `nll_loss` function given the predictions and the target labels as defined in the introduction of this section. To recap, the equation for negative log likelihood is given by:\n",
        "\\begin{equation}\n",
        "L = - \\frac{1}{N} \\sum_{i=1}^N \\log \\hat{y}^{(i)}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:14.212282Z",
          "start_time": "2023-09-25T19:55:14.171357Z"
        },
        "id": "lbxQXl_zF8K-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def nll_loss(prediction: np.ndarray, target: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Computes the negative log likelihood loss between the prediction and the target\n",
        "\n",
        "    Args:\n",
        "        prediction: Numpy array of shape [batch size x num_classes]\n",
        "        target: Numpy array of shape  [batch size, ]\n",
        "\n",
        "    Returns:\n",
        "       (float): the negative log likelihood loss\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = prediction.shape[0]\n",
        "\n",
        "    ...\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ZZHEly4o-QmW"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q3.4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "pmeWQLfsF8LA"
      },
      "source": [
        "**Question 3.5 (6 points)**\n",
        "Using the gradient update expressions that you have derived in Question 3.1, implement the `compute_gradients` function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:17.685936Z",
          "start_time": "2023-09-25T19:55:17.625905Z"
        },
        "id": "6fRru-NNF8LA",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def compute_gradients(x: np.ndarray, prediction: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the gradient of the loss function w.r.t the parameters\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Numpy array of shape [batch size x num_features]\n",
        "        prediction (np.ndarray): Numpy array of shape [batch size x num_classes]\n",
        "        target (np.ndarray): Numpy array of shape  [batch size, ]\n",
        "\n",
        "    Returns:\n",
        "        grad_W (np.ndarray): Numpy array of shape [num_features x num_classes]\n",
        "                             i.e. same as the weights matrix\n",
        "        grad_b (np.ndarray): Numpy array of shape [num_classes, ]\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    ...\n",
        "\n",
        "    return grad_W, grad_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1bCjUFdw-QmW"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q3.5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "JD8iJ-LMF8LC"
      },
      "source": [
        "**Question 3.6 (6 points)**\n",
        "Validation is one of the most important phases in training machine learning models. This is done so as to evaluate the learning capability of the model by testing it on the samples from the validation set. The procedure is as follows: Given the model and the batch size, iterate through the validation set to compute the loss and accuracy of the model. Note that in the validation phase, we do not compute the gradients.  \n",
        "Now, Implement the `validation` function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:20.706884Z",
          "start_time": "2023-09-25T19:55:20.648094Z"
        },
        "id": "pAWobDVCF8LC",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def validation(model: Type[LogisticRegressionModel], val_set: np.ndarray, val_labels: np.ndarray,\n",
        "                batch_size: int) -> float:\n",
        "    \"\"\"\n",
        "    Performs validation of the given input model\n",
        "\n",
        "    Args:\n",
        "        model (type: class): the model to be validated\n",
        "        val_set (np.ndarray): Numpy array of shape [val_size x num_features]\n",
        "        val_labels (np.ndarray): Numpy array of shape [val_size]\n",
        "        batch_size (int): Int defining the batch_size\n",
        "\n",
        "    Returns:\n",
        "        val_loss (float): the validation loss for the entire validation set\n",
        "        val_acc (float): the validation accuracy for the entire validation set\n",
        "    \"\"\"\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    sample_count = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    for batch, labels in iterate_samples(batch_size, val_set, val_labels, False):\n",
        "\n",
        "        ...\n",
        "\n",
        "    validation_loss = total_loss / batch_count\n",
        "    validation_acc = correct_preds / sample_count\n",
        "\n",
        "    return validation_loss, validation_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "YBSMSed--QmX"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q3.6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "-y4Zz5ydF8LE"
      },
      "source": [
        "**Question 3.7 (10 points)**\n",
        "Next, implement the `train_one_epoch` function below. This function uses combines the functions that you have already implemented above, namely, the `LogisticRegressionModel` class, the `nll_loss`, `compute_gradients`, and the `validation` functions. This function returns all the necessary outputs required for plotting the training and validation curves as shall be seen below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:24.021257Z",
          "start_time": "2023-09-25T19:55:23.981629Z"
        },
        "id": "H0bmr_iFF8LF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model: Type[LogisticRegressionModel],\n",
        "                    train_set: np.ndarray, train_labels: np.ndarray,\n",
        "                    val_set: np.ndarray, val_labels:np.ndarray,\n",
        "                    batch_size: int, learning_rate: float,\n",
        "                    validation_every_x_step: int) -> float:\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch on the entire dataset with the given learning rate and batch size\n",
        "\n",
        "    Args:\n",
        "        model (class): the model used to train\n",
        "        train_set (np.ndarray): Numpy array of shape [val_size x num_features]\n",
        "        train_laels (np.ndarray): Numpy array of shape [val_size]\n",
        "        val_set (np.ndarray): Numpy array of shape [val_size x num_features]\n",
        "        val_laels (np.ndarray): Numpy array of shape [val_size]\n",
        "        batch_size (int): the batch size to be used to iterate through the dataset\n",
        "        learning_rate (float): the learning rate to be used for mini-batch gradient descent optimization\n",
        "        validation_every_x_step (int): the number of steps to wait before performing validation\n",
        "\n",
        "    Returns:\n",
        "        train_losses (list): a list of training losses\n",
        "        train_accuracies (list): a list of training accuracies\n",
        "        # train_steps (list): a list of the training batch ids, i.e. each element is the n-th batch of the training set\n",
        "        train_steps (list): a list of the number of training steps. One training step is defined as one forward pass\n",
        "                            (i.e. calculating the loss) AND one backward pass (i.e. calculating the gradients and updating the parameters)\n",
        "                            of a mini-batch of samples through the model\n",
        "        val_losses (list): a list of validation losses\n",
        "        val_accuracies (list): a list of validation accuracies\n",
        "        val_steps (list): a list of the validation steps. One validation step is defined one forward pass of the validaton mini-batch\n",
        "                            samples through the model\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    train_steps = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    val_steps = []\n",
        "    step_count = 0\n",
        "\n",
        "    # Iterate through the training set and append the corresponding metrics to the list\n",
        "    for x_batch, targets in iterate_samples(batch_size, train_set, train_labels, True):\n",
        "        ...\n",
        "\n",
        "        # perform validation depending on the value of `validation_every_x_step`\n",
        "        if (step_count % validation_every_x_step) == 0 or step_count == 1:\n",
        "            ...\n",
        "\n",
        "            val_steps.append(step_count)\n",
        "\n",
        "    return train_losses, train_accuracies, train_steps, val_losses, val_accuracies, val_steps\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "wq4htBQr-QmX"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q3.7\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UMswf-vF8LH"
      },
      "source": [
        "#### Bringing it all together\n",
        "\n",
        "Using all the functions you have implemented above, you will now train a logistic regression model on the  MNIST dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:27.064291Z",
          "start_time": "2023-09-25T19:55:27.020850Z"
        },
        "id": "4XcBKZJKF8LI"
      },
      "outputs": [],
      "source": [
        "def logistic_fit_classifier(num_epochs: int, batch_size: int, learning_rate: float, validation_every_x_step: int, W_initial_weights: float) -> float:\n",
        "    \"\"\"\n",
        "    Trains the logistic regression model\n",
        "\n",
        "    Args:\n",
        "        num_epochs (int): Number of epochs to train the model for\n",
        "        batch_size (int): Size of the mini-batch\n",
        "        learning_rate (float): Step size for mini-batch gradient descent optimization\n",
        "        validation_every_x_step (int): Perform validation at every x-th step\n",
        "        W_initial_weights (float): Randomly initialized weight matrix\n",
        "\n",
        "    Returns:\n",
        "        train_loss: list containing training losses at each epoch\n",
        "        train_accuracy: list containing training accuracies at each epoch\n",
        "        train_step:\n",
        "        val_loss: list containing validation losses at each epoch\n",
        "        val_accuracy: list containing validation accuracies at each epoch\n",
        "        val_step:\n",
        "    \"\"\"\n",
        "\n",
        "    train_loss = []\n",
        "    train_accuracy = []\n",
        "    train_step = []\n",
        "    val_loss = []\n",
        "    val_accuracy = []\n",
        "    val_step = []\n",
        "    epoch_last_step = 0\n",
        "\n",
        "    model = LogisticRegressionModel(W_initial_weights)\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "        epoch_train_loss, epoch_train_accuracy, epoch_train_step, \\\n",
        "             epoch_val_loss, epoch_val_accuracy, epoch_val_step = \\\n",
        "                train_one_epoch(model, train_set, train_labels, val_set,\n",
        "                    val_labels, batch_size, learning_rate,\n",
        "                    validation_every_x_step)\n",
        "\n",
        "        train_loss += epoch_train_loss\n",
        "        train_accuracy += epoch_train_accuracy\n",
        "        train_step += [step + epoch_last_step for step in epoch_train_step]\n",
        "\n",
        "        val_loss += epoch_val_loss\n",
        "        val_accuracy += epoch_val_accuracy\n",
        "        val_step += [step + epoch_last_step for step in epoch_val_step]\n",
        "\n",
        "        epoch_last_step = train_step[-1]\n",
        "\n",
        "    return train_loss, train_accuracy, train_step, val_loss, val_accuracy, val_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:43.918415Z",
          "start_time": "2023-09-25T19:55:27.368854Z"
        },
        "id": "7Dxda69iF8LJ"
      },
      "outputs": [],
      "source": [
        "# initiliaze weights from a normal distribution\n",
        "W_initial_weights = np.random.normal(0.5, 0.1, (784, 10))\n",
        "\n",
        "# train a logistic regression model\n",
        "train_loss_bs100, train_accuracy_bs100, train_step_bs100, val_loss_bs100, \\\n",
        "    val_accuracy_bs100, val_step_bs100 = \\\n",
        "        logistic_fit_classifier(num_epochs=4, batch_size=100, learning_rate=0.1,\n",
        "        validation_every_x_step=10, W_initial_weights=W_initial_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "cysgKkacF8LL"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question 3.8 (2 points):** As in the previous section, you are asked to plot the training and validation accuracy curves (points are associated to the type of the curves you report)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:44.058115Z",
          "start_time": "2023-09-25T19:55:43.919155Z"
        },
        "deletable": false,
        "editable": false,
        "id": "QrLwV7IlF8LL"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(train_step_bs100, train_accuracy_bs100, label='train_accuracy')\n",
        "plt.plot(val_step_bs100, val_accuracy_bs100, label='val_accuracy')\n",
        "plt.xlabel(\"Num Steps\", fontsize=14)\n",
        "plt.ylabel(\"Test Acc.\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "u0PqbT0H-QmY"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "We will be using a batch size of 100 and a learning rate of 0.1 to evaluate the logistic regression classifier on the test set. Ideally, extensive hyperparamter tuning must be done on the validation set to choose the best possible hyperparameter configurations for your classifier. However, one of your objectives in the next assignment will be hyperparameter tuning, hence we are giving you the best hyperparameters in this assignment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:55:44.108871Z",
          "start_time": "2023-09-25T19:55:44.050912Z"
        },
        "id": "qIBZoqUyF8LL"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL TO DEFINE THE FUNCTION FOR TESTING THE MODEL\n",
        "# NOTE: IT IS VERY SIMILAR TO `train_model` THAT YOU HAVE ALREADY IMPLEMENTED EXCEPT, THE 'VALIDATION' PARTS ARE CHANGED TO 'TEST'\n",
        "from tqdm import tqdm\n",
        "\n",
        "def test_model(num_epochs: int, batch_size: int, learning_rate: float, validation_every_x_step: int, W_initial_weights: float) -> float:\n",
        "    \"\"\"\n",
        "    Trains the logistic regression model\n",
        "\n",
        "    Args:\n",
        "        num_epochs (int): Number of epochs to train the model for\n",
        "        batch_size (int): Size of the mini-batch\n",
        "        learning_rate (float): Step size for mini-batch gradient descent optimization\n",
        "        validation_every_x_step (int): Perform validation at every x-th step\n",
        "        W_initial_weights (float): Randomly initialized weight matrix\n",
        "\n",
        "    Returns:\n",
        "        train_loss: list containing training losses at each epoch\n",
        "        train_accuracy: list containing training accuracies at each epoch\n",
        "        train_step:\n",
        "        test_loss: list containing test losses at each epoch\n",
        "        test_accuracy: list containing test accuracies at each epoch\n",
        "        test_step:\n",
        "    \"\"\"\n",
        "\n",
        "    train_loss = []\n",
        "    train_accuracy = []\n",
        "    train_step = []\n",
        "    test_loss = []\n",
        "    test_accuracy = []\n",
        "    test_step = []\n",
        "    epoch_last_step = 0\n",
        "\n",
        "    model = LogisticRegressionModel(W_initial_weights)\n",
        "\n",
        "    # note here that we have just replaced the validation set with the test set,\n",
        "    # the rest of the procedure remains the same\n",
        "    for i in tqdm(range(num_epochs)):\n",
        "        epoch_train_loss, epoch_train_accuracy, epoch_train_step, \\\n",
        "             epoch_test_loss, epoch_test_accuracy, epoch_test_step = \\\n",
        "                train_one_epoch(model, train_set, train_labels, test_set,\n",
        "                    test_labels, batch_size, learning_rate,\n",
        "                    validation_every_x_step)\n",
        "\n",
        "        train_loss += epoch_train_loss\n",
        "        train_accuracy += epoch_train_accuracy\n",
        "        train_step += [step + epoch_last_step for step in epoch_train_step]\n",
        "\n",
        "        test_loss += epoch_test_loss\n",
        "        test_accuracy += epoch_test_accuracy\n",
        "        test_step += [step + epoch_last_step for step in epoch_test_step]\n",
        "\n",
        "        epoch_last_step = train_step[-1]\n",
        "\n",
        "    return train_loss, train_accuracy, train_step, test_loss, test_accuracy, test_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T19:56:28.343576Z",
          "start_time": "2023-09-25T19:55:44.070135Z"
        },
        "id": "JGMi8PO5F8LM"
      },
      "outputs": [],
      "source": [
        "best_batch_size = 100\n",
        "best_lr = 0.1\n",
        "\n",
        "# Test the model!\n",
        "_ , _ , _ , \\\n",
        "     test_loss, test_accuracy, test_step = test_model(num_epochs=100, batch_size=best_batch_size,\n",
        "                                                        learning_rate=best_lr, validation_every_x_step=100, W_initial_weights=W_initial_weights)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(test_step, test_accuracy, label=f\"bs={best_batch_size}, lr={best_lr}\")\n",
        "plt.title('Test Accuracy')\n",
        "plt.ylim(0.6, 0.95)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best test accuracy: {test_accuracy[-1] * 100.0} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "X-lNJPHr-QmY"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question 3.9 (1 point):** Report the test accuracy obtained by the Logistic regression. (1 sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "G7O5iio7-QmY"
      },
      "source": [
        "**Answer 3.9:** The test accuracy obtained is .... %."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "GdwbfzRI1JKu"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "### 4. Comparing kNN, GNB and Logistic Regression Classifiers (5 points)\n",
        "\n",
        "Nice work! Now that your logistic regression classifier works, we will compare the performance of these 3 classifiers together. We will also compare the performance of above classifiers on FashionMNIST dataset.\n",
        "\n",
        "Fashion MNIST typically acts a drop-in replacement for the well-known MNIST dataset. Its structure is quite similar to MNIST except the labels now represent fashion categories (instead of digits). Here's a table representing each category:\n",
        "\n",
        "| Label      | Description |\n",
        "| ----------- | ----------- |\n",
        "| 0      | T-shirt/Top       |\n",
        "| 1   | Trouser        |\n",
        "| 2   | Pullover        |\n",
        "| 3   | Dress        |\n",
        "| 4   | Coat        |\n",
        "| 5   | Sandal        |\n",
        "| 6   | Shirt        |\n",
        "| 7   | Sneaker        |\n",
        "| 8   | Bag        |\n",
        "| 9   | Ankle Boot        |\n",
        "It is always important to visualize the data. In the next cell, we load the FashionMNIST dataset and plot 10 images from the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nwwa38ip1JKu"
      },
      "outputs": [],
      "source": [
        "def plot_fashion_mnist(images, labels, n_row=2, n_col=5):\n",
        "  fig, axes = plt.subplots(n_row, n_col, figsize=(10, 5))\n",
        "  for i in range(n_row * n_col):\n",
        "      ax = axes[i//n_col, i%n_col]\n",
        "      ax.imshow(images[i], cmap='gray')\n",
        "      ax.set_title('Label: {}'.format(labels[i]))\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLwMLcJQ1JKu"
      },
      "outputs": [],
      "source": [
        "# Load Fashion MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Shuffle the training set\n",
        "permuted = np.random.permutation(len(x_train))\n",
        "x_train, y_train = x_train[permuted], y_train[permuted]\n",
        "\n",
        "print(f\"Number of images for training: {x_train.shape[0]}\")\n",
        "print(f\"Number of images for testing: {x_test.shape[0]}\")\n",
        "print(f\"Size of Fashion MNIST images: {x_train[0].shape}\")\n",
        "\n",
        "# Visualize 10 images from the training set\n",
        "plot_fashion_mnist(images=x_train[:10], labels=y_train[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "yh1vDqa31JKu"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "pyzKCZF51JKw"
      },
      "source": [
        "Now, normalize the training and test data with the function defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-09-25T22:31:01.403353Z",
          "start_time": "2023-09-25T22:31:01.289381Z"
        },
        "id": "hzphEN8c1JKw"
      },
      "outputs": [],
      "source": [
        "# NOTE: We normalize the training data by dividing the input by max value.\n",
        "max_value = np.max(x_train)\n",
        "\n",
        "train_images = x_train / max_value\n",
        "test_images = x_test / max_value\n",
        "\n",
        "# Vectorize the data\n",
        "train_images = train_images.reshape((train_images.shape[0], -1))\n",
        "test_images = test_images.reshape((test_images.shape[0], -1))\n",
        "\n",
        "print(f\"Training inputs' shape after vectorization: {train_images.shape}\")\n",
        "print(f\"Testing inputs' shape after vectorization: {test_images.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qFhH7L1D1JKw"
      },
      "source": [
        "Now, using these functions, we will run the k-NN classifier on a subset of the FashionMNIST dataset! Particularly, we will use 50,000 samples for training, 10,000 samples for validation, and 10,000 samples for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0419N4rX1JKw"
      },
      "outputs": [],
      "source": [
        "n_train_samples = 50000\n",
        "n_val_samples = 10000\n",
        "n_test_samples = 10000\n",
        "\n",
        "# define the training set and labels\n",
        "train_set = train_images[:n_train_samples]\n",
        "train_labels = y_train[:n_train_samples]\n",
        "print(f\"Training set shape: {train_set.shape}\")\n",
        "\n",
        "# define the validation set and labels\n",
        "val_set = train_images[-n_val_samples:]\n",
        "val_labels = y_train[-n_val_samples:]\n",
        "print(f\"Validaton set shape: {val_set.shape}\")\n",
        "\n",
        "# define the test set and labels\n",
        "test_set = test_images[:n_test_samples]\n",
        "test_labels = y_test[:n_test_samples]\n",
        "print(f\"Test set shape: {test_set.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "7-JvQzvh1JKw"
      },
      "source": [
        "#### Test all three classifiers on FashionMNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sccvd6bm1JKw"
      },
      "outputs": [],
      "source": [
        "# kNN classifier (let's use the same best_k value we obtained for MNIST dataset)\n",
        "knn_test_accuracy = knn_classifier(train_set, train_labels, test_set, test_labels, k=best_k)\n",
        "print(f\"\\nkNN classifier: {knn_test_accuracy} %\")\n",
        "\n",
        "\n",
        "# GNB classifier\n",
        "gnb_test_accuracy = gnb_classifier(train_set, train_labels, test_set, test_labels)\n",
        "print(f\"\\nCNB classifier: {gnb_test_accuracy} %\")\n",
        "\n",
        "# Logistic regression\n",
        "_, _, _,  _, log_test_accuracy, _ = test_model(num_epochs=100, batch_size=100,\n",
        "                                             learning_rate=0.1, validation_every_x_step=100,\n",
        "                                             W_initial_weights=W_initial_weights)\n",
        "print(f\"\\nLogistic regression: {log_test_accuracy[-1] * 100.0} %\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "81Q5fxlA1JKw"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question 4.1:** Report the test accuracies obtained using all the three classifiers on both MNIST and FashionMNIST results. Which classifier performs the best overall? Why do you think this is the case? (Justify in 3-4 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gD6f6CNY1JKx"
      },
      "source": [
        "**Answer 4.1:**\n",
        "kNN accuracy = .... %, GNB accuracy = .... %, and Logistic regression accuracy = .... % (with batch_size = ...., lr = ....)\n",
        "\n",
        "Out of the three, ..... classifier performs the best.\n",
        "\n",
        "This is because ....."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "kSyOp8qh-QmZ"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "## **Poission Regression (15 points)**\n",
        "\n",
        "In logistic regression, the assumption was that the binary label $Y_i \\in \\{0,1\\}$ followed a Bernoulli distribution, in other words $\\Pr(Y_i = 1 | X_i) = p_i$, where $p_i$ is the mean of the distribution. Under the independence assumption, the log-likelihood function could be derived as being\n",
        "\\begin{align}\n",
        "\\sum_{i=1}^n (1-y_i) \\log(1-p_i) + y_i \\log(p_i).\n",
        "\\end{align}\n",
        "Then using the logit transformation, the parameterization we achieved was:\n",
        "\\begin{align}\n",
        "\\log\\frac{p_i}{1-p_i} = {\\bf x}_i . {\\bf w} + b, \\quad \\mbox{ or equivalently } \\quad p_i = \\frac{1}{1+\\exp(-{\\bf x}_i . {\\bf w} - b)}.\n",
        "\\end{align}\n",
        "Furthemore, the weight vector and bias could both be found through maximizing the log-likelihood function.\n",
        "\n",
        "The following problem attempts to generalize this to the case where $Y_i \\in \\mathbb{N}$, meaning that the outcome could be any natural number.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "aO5IJxRb-QmZ"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "### **5.1 (5 points)**\n",
        "\n",
        "Assume that each $Y_i \\in \\mathbb{N}$ follows the Poisson distribution (with mean $\\mu_i \\geq 0$):\n",
        "\t\\begin{align}\n",
        "\t\\Pr(Y_i = k | X_i) = \\frac{\\mu_i^k}{k!} \\exp(-\\mu_i), ~~ k = 0, 1, 2, \\ldots\n",
        "\t\\end{align}\n",
        "Given a dataset $\\mathcal{D} = \\{{\\bf x}_i, y_i\\}_{i=1}^n$, what is the log-likelihood function (of $\\mu_i$'s) given $\\mathcal{D}$?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aSoEPKdY-QmZ"
      },
      "source": [
        "**Answer 5.1:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "6M7SnE9D-QmZ"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "### **5.2 (2 points)**\n",
        "Can you give some justification of the parameterization below?\n",
        "\t\\begin{align}\n",
        "\t\\log\\mu_i = {\\bf x}_i . {\\bf w} + b.\n",
        "\t\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "t3azpf3Q-Qma"
      },
      "source": [
        "**Answer 5.2:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "0HS7XVAN-Qma"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "### **5.3 (3 points)**\n",
        "Using the solution to the above, write down the objective function for Poisson regression. Please specify the optimization variables and whether you are maximizing or minimizing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "BaH2xvDq-Qma"
      },
      "source": [
        "**Answer 5.3:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "deletable": false,
        "editable": false,
        "id": "5_4CdYOt-Qma"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "### **5.4 (5 points)**\n",
        "Compute the gradient of your objective function above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "c9fxn2EP-Qma"
      },
      "source": [
        "**Answer 5.4:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "vVnIKpdY-Qma"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.8 (main, Nov 20 2021, 01:19:14) \n[Clang 10.0.1 (clang-1001.0.46.4)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}