{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "## Instructions\n",
    "This is an individual assignment. You are not allowed to discuss the problems with other students.\n",
    "\n",
    "- Part of this assignment will be autograded by Gradescope. You can use it as immediate feedback to improve your answers. You can resubmit as many times as you want. We provide some tests which you can use to check that your code will execute properly on Gradescope. These are **not** meant to check the correctness of your answers. We encourage you to write your own tests for this.\n",
    "- All your code, analysis, graphs, explanations, etc. should be done in this same notebook.\n",
    "- Please make sure to execute all the cells before you export the notebook to a PDF and submit it to Gradescope. You will not get points for the plots if they are not generated already.\n",
    "- If you have questions regarding the assignment, you can ask for clarifications in Piazza. You should use the corresponding tag for this assignment.\n",
    "\n",
    "Before starting the assignment, make sure that you have downloaded all the data and tests related for the assignment and put them in the appropriate locations. If you run the next cell, we will set this all up automatically for you in a dataset called public, which will contain both the data and tests you use.\n",
    "\n",
    "**Warning**: Throughout the assignment, you will be asked to implement certain algorithms and find optimal values. In the solution you submit, do not simply call a library function which performs the entire algorithm for you, this is forbidden, as it would obviously defeat the purpose. For example, if you were asked to implement the linear regression, do not simply call an outside package for help.\n",
    "\n",
    "**When Submitting to GradeScope**: Be sure to\n",
    "1) Submit a `.ipynb` notebook to the `Assignment 1 - Practial` section on Gradescope.\n",
    "2) Submit a `pdf` version of the notebook to the `Assignment 1 - Analysis` entry.\n",
    "\n",
    "**Note**: You can choose to submit responses in either English or French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q otter-grader\n",
    "!git clone https://github.com/chandar-lab/INF8245e-assignments-2023.git public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import otter\n",
    "grader = otter.Notebook(colab=True, tests_dir='./public/a1/tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RE-SEED ANYWHERE OTHERWISE YOUR IMPLEMENTATION MAY FAIL SOME UNIT TESTS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "np.random.seed(8245)\n",
    "%matplotlib inline\n",
    "\n",
    "working_dir = './public/a1' # CHANGE THIS TO THE MAIN ASSIGNMENT DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivjv-2OB1hwZ"
   },
   "source": [
    "## **Question 1: Linear Regression (30 points)**\n",
    "In this question, you will take a simple dataset and implement linear and ridge regression by solving for their analytical solutions. You will then perform a simple hyperparameter search to determine regression co-efficients that best suit the data.\n",
    "\n",
    "We'll be using the Boston Housing Dataset, which you can find out more about [here](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). The goal is to determine the best way to weight different features of houses to determine the price it was sold for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tW1m54qm2O8J"
   },
   "source": [
    "The following cell is just for loading the data properly in the format you need. In this case, we load our data as a design matrix $X$ where\n",
    "$$\n",
    "X = \\begin{bmatrix} x_{11} & \\cdots & x_{1d} \\\\ \\vdots & \\ddots & \\vdots \\\\ x_{n1} & \\cdots & x_{nd} \\end{bmatrix}\n",
    "$$ where each row is an instance of the data. Meanwhile, the targets $y$ is loaded as a single column vector where the $i$-th entry corresponds to the target for the $i$-th row in $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZwbIVgM2PT-"
   },
   "outputs": [],
   "source": [
    "def load_linear_regression_data(working_dir):\n",
    "    test_inputs = np.genfromtxt(working_dir + '/data/housing_X_test.csv', delimiter=',')\n",
    "    test_targets = np.genfromtxt(working_dir + '/data/housing_y_test.csv', delimiter=',')\n",
    "    train_inputs = np.genfromtxt(working_dir + '/data/housing_X_train.csv', delimiter=',')\n",
    "    train_targets = np.genfromtxt(working_dir + '/data/housing_y_train.csv', delimiter=',')\n",
    "\n",
    "    return train_inputs.T, train_targets.reshape((train_targets.shape[0], 1)), test_inputs.T, test_targets.reshape((test_targets.shape[0], 1))\n",
    "\n",
    "X_tr, y_tr, X_te, y_te = load_linear_regression_data(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWxdsFMu1tAV"
   },
   "source": [
    "### Q1a) **(10 points)**\n",
    "\n",
    "The following functions are helper functions meant to help with processing all the data and running your functions. However they will be important for the rest of your assignment so be sure to implement them correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIONVEB33dSz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_inputs_with_bias(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (np.ndarray): Input\n",
    "\n",
    "    Returns:\n",
    "        X_new (np.ndarray): Input with biases appended\n",
    "    \"\"\"\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UvpE0IN4kl4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmse(y: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y (np.ndarray): True labels/values\n",
    "        y_hat (np.ndarray): Predicted labels/values\n",
    "\n",
    "    Returns:\n",
    "        float: Root Mean-Squared Error\n",
    "    \"\"\"\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDJnXWDA55uI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_linear_regression(inputs: np.ndarray, weights: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (np.ndarray): Input\n",
    "        weights (np.ndarray): Weight vector\n",
    "\n",
    "    Returns:\n",
    "        prediced_values: np.ndarray: Predictions\n",
    "    \"\"\"\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tW4RwAb65dM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def solve_linear_regression(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (np.ndarray): Input\n",
    "        y (np.ndarray): Labels\n",
    "\n",
    "    Returns:\n",
    "        w_sol (np.ndarray): Analytical solution weight vector\n",
    "    \"\"\"\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jii57VUw9zyo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def solve_ridge_regression(X: np.ndarray, y: np.ndarray, lambda_hyperparameter: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (np.ndarray): Input\n",
    "        y (np.ndarray): Labels\n",
    "        lambda_hyperparameter (float): Regularization coefficient\n",
    "\n",
    "    Returns:\n",
    "        w_sol (np.ndarray): Analytical solution weight vector\n",
    "    \"\"\"\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qusFUlRb7HgO"
   },
   "source": [
    "### Q1b) **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzXfC2A_MxDd"
   },
   "source": [
    "Now that we have all of our main functions written out correctly, we would like to perform a hyperparameter search on ridge regression. In particular, we want to find the best regularization co-efficient $\\lambda$ based on the housing dataset we currently have. If you recall, ridge regression is a regularized version of linear regression where is loss is calculated as\n",
    "$$\n",
    "\\tilde{L}(X, y, w) = L(X, y, w) + \\lambda \\cdot \\|w\\|_2^2\n",
    "$$\n",
    "where $L$ is the loss for linear regression and $w$ is the current weight vector.\n",
    "\n",
    "Here, we'll perform a $k$-fold cross validation over the samples to estimate the best value of $\\lambda$ that enables best transfer from the training dataset to the testing dataset. For those who are still unfamiliar with $k$-fold CV, the idea is to duplicate the training dataset $k$ times, then for each duplicate split the dataset into a training and validation set. For each duplicate, or fold, the validation set should be different from all other folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUrX4QDtPb6M"
   },
   "source": [
    "Since there are many ways to perform the splitting, we'll be doing it in a very simple manner. To avoid any random seeding issues, we'll be creating each fold's train/validation sets deterministically. \n",
    "\n",
    "First, calculate a `fold_size` by determining the best size which splits the trianing set evenly between the desired number of folds. Leave this value as a decimal as rounding will be taken care of later.\n",
    "\n",
    "Then for the $i$-th fold, simply use the `fold_size * i` to `fold_size * (i+1)` entries of the training data as your validation set. To avoid confusion, if either value is not an integer, simply round the values to the nearest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTLzWhQxHQte",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_validation_linear_regression(k_folds: int, hyperparameters: List[float],\n",
    "                                       X: np.ndarray, y: np.ndarray) -> Tuple[float, float, List[float]]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        k_folds (int): Number of folds to use\n",
    "        hyperprameters (np.ndarray): Numpy array of floats containing the hyperparameter values to search\n",
    "        X (np.ndarray): Numpy array of shape [observations, features]\n",
    "        y (np.ndarray): Numpy array of shape [observations, 1]\n",
    "\n",
    "    Returns:\n",
    "        best_hyperparam: np.float value corresponding to the best hyperparameter value\n",
    "        best_mean_squared_error: np.float value corersponding to the best RMSE value\n",
    "        mean_squared_errors: np.ndarray of size len(hyperparameters) containing the corresponding RMSE for reach hyperparameter value\n",
    "    \"\"\"\n",
    "\n",
    "    best_hyperparam = 0.0\n",
    "    best_mean_squared_error = 0.0\n",
    "    mean_squared_errors = np.zeros(len(hyperparameters))\n",
    "\n",
    "    ...\n",
    "\n",
    "    return best_hyperparam, best_mean_squared_error, mean_squared_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjXEBEeSOW9H"
   },
   "source": [
    "### Q1c) **(5 points)**\n",
    "Now perform a hyperparameter search on the $\\lambda$ regularization coefficient using the following values. What is the best hyperparameter value you can find based on this $k$-fold cross validation? Does it perform well on the test set as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZfvR05AH7WR",
    "outputId": "ef6ae36b-df68-4ee6-e160-4771c26b0891",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lambda values to be evaluated by cross validation\n",
    "hyperparams = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2,\n",
    "               1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4,\n",
    "               2.5, 2.6, 2.7, 2.8, 2.9, 3.0]\n",
    "k_folds = 10\n",
    "\n",
    "\"\"\"\n",
    "Write code here to perform the cross-validation and find the best hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "best_lambda = 0\n",
    "best_mean_squared_error = 0\n",
    "mean_squared_error = 0\n",
    "\n",
    "...\n",
    "\n",
    "print('best lambda: ' + str (best_lambda))\n",
    "print('best cross validation mean squared error: ' + str(best_mean_squared_error))\n",
    "print('test mean squared error: ' + str(mean_squared_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "v7xAXnhPcnP6"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Q1d) **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEg49OVTOoIJ"
   },
   "source": [
    "Plot both the validation and test mean squared errors as a function of the $\\lambda$ hyperparameter for ridge regression.\n",
    "\n",
    "Is the hyperparameter you found from the cross validation the best choice? Are large values of $\\lambda$ problematic? Why? What about small values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "v5-MqGTjKwfA",
    "outputId": "f879c7c1-1c15-4b77-b789-64eaf123b9c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_linear_regression_mean_squared_errors(mean_squared_errors,hyperparams):\n",
    "    plt.plot(hyperparams,mean_squared_errors)\n",
    "    plt.ylabel('mean squared error')\n",
    "    plt.xlabel('lambda')\n",
    "    plt.show()\n",
    "\n",
    "# plot results\n",
    "plot_linear_regression_mean_squared_errors(mean_squared_errors,hyperparams)\n",
    "\n",
    "test_mse = np.zeros(len(hyperparams))\n",
    "\n",
    "\"\"\"\n",
    "Write code to find the test MSE for all hyperparameters\n",
    "\"\"\"\n",
    "...\n",
    "\n",
    "plot_linear_regression_mean_squared_errors(test_mse, hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "K9IDJH7eQPLu"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## **Question 2: Gradient Descent (55 points)**\n",
    "At this point, we should know how to solve regression tasks using an analytical solution. However, not all types of methods have something nice to calculate like linear or ridge regression. In this section, we'll learn to use _gradient descent_, as you've seen in class, to solve regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INSBOBxAQWzm"
   },
   "source": [
    "We'll now use a synthetically generated dataset for this part of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gd_data(working_dir):\n",
    "    test_inputs = np.genfromtxt(working_dir + '/data/test_inputs.csv', delimiter=',')\n",
    "    test_targets = np.genfromtxt(working_dir + '/data/test_targets.csv', delimiter=',')\n",
    "    train_inputs = np.genfromtxt(working_dir + '/data/train_inputs.csv', delimiter=',')\n",
    "    train_targets = np.genfromtxt(working_dir + '/data/train_targets.csv', delimiter=',')\n",
    "\n",
    "    return train_inputs, train_targets.reshape((train_targets.shape[0], 1)), test_inputs, test_targets.reshape((test_targets.shape[0], 1))\n",
    "\n",
    "X_tr, y_tr, X_te, y_te = load_gd_data(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xOJC2aEQwVR"
   },
   "source": [
    "### Q2a) **(15 points)**\n",
    "This part of the question will deal with implementing functions that can calculate the gradients that are used during regression.\n",
    "\n",
    "Along with linear and ridge regression, we introduce LASSO regression, which is similar to ridge regression but instead of using a penalty term of $\\|w\\|_2^2$, we use a penalty of $\\|w\\|_1$. Since you might not be familiar with this notation, we denote the function $f(\\cdot) = \\| \\cdot \\|_1$ as the L1-norm function, where\n",
    "$$\n",
    "\\| w \\|_1 = |w_1| + |w_2| + \\dots + |w_d|\n",
    "$$\n",
    "\n",
    "Provide a function that computes the gradient for all three regression types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgOxrP9aQ4Qu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_regression_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    \"\"\"Compute the gradient of w with respect to the loss\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Numpy array of shape [observations, features]\n",
    "        y (np.ndarray): Numpy array of shape [observations, 1]\n",
    "        w (np.ndarray): Numpy array of shape [features, 1], where the last value is w_0 and all other values represent w_1\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Gradient of w with respect to the loss, as a numpy array of the same shape as w.\n",
    "    \"\"\"\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jiLGhi_aQeAO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ridge_regression_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray, hyperparameter: float, **kwargs) -> np.ndarray:\n",
    "    \"\"\"Compute the gradient of w with respect to the loss\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Numpy array of shape [observations, features]\n",
    "        y (np.ndarray): Numpy array of shape [observations, 1]\n",
    "        w (np.ndarray): Numpy array of shape [features, 1], where the last value is w_0 and all other values represent w_1\n",
    "        hyperparameter (float): Lambda used in L2 regularizer\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Gradient of w with respect to the loss, as a numpy array of the same shape as w.\n",
    "    \"\"\"\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBV4LLV7OuAA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lasso_regression_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray, hyperparameter: float, **kwargs) -> np.ndarray:\n",
    "    \"\"\"Compute the gradient of w with respect to the loss\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Numpy array of shape [observations, features]\n",
    "        y (np.ndarray): Numpy array of shape [observations, 1]\n",
    "        w (np.ndarray): Numpy array of shape [features, 1], where the last value is w_0 and all other values represent w_1\n",
    "        hyperparameter (float): Lambda used in L2 regularizer\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Gradient of w with respect to the loss, as a numpy array of the same shape as w.\n",
    "    \"\"\"\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-5QCSFQRDLa"
   },
   "source": [
    "### Q2b) **(15 points)**\n",
    "\n",
    "Now write a function for solving a given regression task using gradient descent instead of the analytical solution. In this case, the function will take in a parameter `reg_type` to determine the type of regression we're using, a `hyperparameter` in case we are doing regularization, a `learning_rate` parameter which sets the size of the steps we make in the gradient directions and `num_epochs` which defines how many gradient descent steps we wish to perform.\n",
    "\n",
    "Throughout the process, you should keep track of the training and testing losses (use your functions implemented in the prior question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Vm5ancYRGyK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def regression_gradient_descent(X_train: np.ndarray, y_train: np.ndarray,\n",
    "                                X_test: np.ndarray, y_test: np.ndarray,\n",
    "                                w_0: np.ndarray, hyperparameter: float,\n",
    "                                learning_rate: float, num_epochs: int, reg_type: str = 'linear') -> Tuple[List[float], List[float], np.ndarray]:\n",
    "    \"\"\" Runs gradient descent to optimize the parameters w.\n",
    "\n",
    "    Args:\n",
    "        y_train (np.ndarray): Numpy array of shape [observations, 1]\n",
    "        X_train (np.ndarray): Numpy array of shape [observations, features]\n",
    "        X_test (np.ndarray): Numpy array of shape [observations, features]\n",
    "        y_test (np.ndarray): Numpy array of shape [observations, 1]\n",
    "        w_0 (np.ndarray): Numpy array of shape [features, 1]\n",
    "        hyperparameter (float): Lambda used in L2 regularizer\n",
    "        learning_rate (float): Value to multiply our gradient with before updating our parameters.\n",
    "        num_steps (int): Number of gradient descent steps to perform.\n",
    "        type (str): The type of regression to use ['linear', 'ridge', 'lasso']\n",
    "\n",
    "    Returns:\n",
    "        train_loss (np.ndarray): The train RMSE loss without L2 regularizer, for each step.\n",
    "        val_loss (np.ndarray): The validation RMSE loss without L2 regularizer, for each step.\n",
    "        optimized_w (np.ndarray): The optimized w parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    valid = {'linear', 'ridge', 'lasso'}\n",
    "    if reg_type not in valid:\n",
    "        raise ValueError(\"Invalid regression type\")\n",
    "\n",
    "    train_losses = np.zeros((num_epochs,), dtype=np.float32)\n",
    "    test_losses = np.zeros((num_epochs,), dtype=np.float32)\n",
    "    w_sol = np.copy(w_0)\n",
    "\n",
    "\n",
    "    ...\n",
    "\n",
    "    return (train_losses, test_losses, w_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-W5fdgvpS9cl"
   },
   "source": [
    "### Q2c) **(5 points)**\n",
    "\n",
    "Now run the above function for both linear and ridge regression. Initialize the initial weight vector using `np.random.normal`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Z2lCoFxcb2Fo"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Your Answer Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Write code to find the solution weights for each type of regression. After training each, plot the training and test losses for each regression type to compare learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDKouH97dauj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 250\n",
    "hyperparameter = 0.15\n",
    "learning_rate = 0.01\n",
    "\n",
    "# add bias at the end of each data point\n",
    "train_inputs = create_inputs_with_bias(X_tr)\n",
    "test_inputs = create_inputs_with_bias(X_te)\n",
    "\n",
    "w_0 = np.random.normal(size=(train_inputs.shape[1], 1))\n",
    "\n",
    "\"\"\"\n",
    "Write code here to find the solution for each method.\n",
    "\"\"\"\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Linear Regression')\n",
    "plt.plot(train_losses_ridge, label='Ridge Regression')\n",
    "plt.plot(train_losses_lasso, label='Lasso Regression')\n",
    "plt.ylabel('mean squared error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training Losses per Epoch')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(test_losses, label='Linear Regression')\n",
    "plt.plot(test_losses_ridge, label='Ridge Regression')\n",
    "plt.plot(test_losses_lasso, label='Lasso Regression')\n",
    "plt.ylabel('mean squared error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Test Losses per Epoch')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Q2d) **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "In the previous examples, we perform what we call full-batch gradient descent. However, this isn't always realistic, as sometimes there is too much data for this to be feasible on modern computers. In these cases, a more feasible option be to perform batch gradient descent, which is a scenario where we have to take a subset of the whole dataset at each gradient update to get an estimate of the direction of the true gradient.\n",
    "\n",
    "There is a simple way in which one can use batches to exhibit the exact behaviour as full-batch gradient descent. In less than 100 words, describe it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6h8E4CcTSLiE"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Q2e) **(5 points)**\n",
    "For the following question, we'll change up the data to a new synthetic dataset that was generated by us. We again load the data in the format where each row of $X$ constitutes an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wb6lyWf2eorr"
   },
   "outputs": [],
   "source": [
    "def load_comp_data(working_dir):\n",
    "    test_inputs = np.genfromtxt(working_dir + '/data/X_test_C.csv', delimiter=',')\n",
    "    test_targets = np.genfromtxt(working_dir + '/data/Y_test_C.csv', delimiter=',')\n",
    "    train_inputs = np.genfromtxt(working_dir + '/data/X_train_C.csv', delimiter=',')\n",
    "    train_targets = np.genfromtxt(working_dir + '/data/Y_train_C.csv', delimiter=',')\n",
    "\n",
    "    return train_inputs, train_targets.reshape((train_targets.shape[0], 1)), test_inputs, test_targets.reshape((test_targets.shape[0], 1))\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_comp_data(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Using a $\\lambda$ (regularization hyperparameter) of 1 for the regularization coefficient, plot the weight distribution for both ridge and Lasso regression.\n",
    "\n",
    "What do you notice about the weights and their values? What about the regularization term do you think is causing this and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "CVgik-Z3SrkS",
    "outputId": "36cbcfd6-cff1-43ee-d999-4345198efa82",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write code to create the data and then run regression on the two.\n",
    "\"\"\"\n",
    "\n",
    "train_inputs = create_inputs_with_bias(X_train)\n",
    "test_inputs = create_inputs_with_bias(X_test)\n",
    "\n",
    "np.random.seed(8245)\n",
    "w_0 = np.random.normal(size=(train_inputs.shape[1], 1))\n",
    "\n",
    "...\n",
    "\n",
    "data = [w_sol_ridge.flatten(), w_sol_lasso.flatten()]\n",
    "labels = ['ridge10', 'lasso10']\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.hist(data, label=labels)\n",
    "\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.title('Histogram of Model Parameter Values')\n",
    "plt.ylabel('Occurrences')\n",
    "plt.xlabel('Parameter Value')\n",
    "plt.show()\n",
    "\n",
    "data = [w_sol_ridge.flatten(), w_sol_lasso.flatten()]\n",
    "labels = ['ridge10', 'lasso10']\n",
    "\n",
    "bins = np.linspace(-1, 1, 20)\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.hist(data, bins = bins, label=labels)\n",
    "\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.title('Histogram of Model Parameter Values')\n",
    "plt.ylabel('Occurrences')\n",
    "plt.xlabel('Parameter Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ktSFcX7CWy_i"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Q2f) **(5 points)**\n",
    "\n",
    "Provide an intuitive manner to understand why this happens mathematically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Q2g) **(5 points)**\n",
    "\n",
    "In Q2d), you derived an algorithm to calculate the full-batch gradient update using only subsets of the data.\n",
    "\n",
    "Consider the case where you perform a gradient update at every batch. In this case, the direction of the gradient update is only an estimate of the true gradient direction. There is a signficant problem we may face if we were to try using this in our above implementation, in particular our learning rate. Can you think of what it is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
